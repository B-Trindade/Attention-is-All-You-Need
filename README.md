**Attention Is All You Need: Reproduction and Explanation Repository**

![Cover Slide](./presentation/imgs/cover/first%20slide%20attention.png)

![First Slide of Presentation](./presentation/SeminÃ¡rio___Attention_is_All_You_Need.pdf#page=1)

---

## ğŸ“– Overview (English)

This repository provides:

1. **Reproduction** of the Transformer architecture from the seminal paper [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762) using PyTorch.
2. **Detailed explanations** of each component (Multi-Head Self-Attention, Positional Encoding, Feed-Forward sublayers, Residual Connections, Layer Normalization).
3. **Jupyter Notebooks** with step-by-step code cells, mathematical derivations in LaTeX, and visualizations of attention weights.
4. **Complementary Presentation** in PDF format; first slide shown above.
5. **Additional References**â€”video lectures, popular science deep-dives, and textbook chapters to deepen understanding.

### ğŸ”— Presentation

* PDF: [Download the complementary presentation](./presentation/Transformer_Presentation.pdf)

### ğŸš€ Getting Started

```bash
# Clone repository
git clone https://github.com/B-Trindade/Attention-is-All-You-Need.git
cd Attention-is-All-You-Need

# Create and activate virtual environment 
python -m venv .venv
source .venv/bin/activate
```

### âš™ï¸ Dependency Installation

Before you begin, ensure you install all required Python packages listed in `requirements.txt`. We strongly recommend doing this inside your activated virtual environment for reproducibility:

```bash
pip install -r requirements.txt
```

### ğŸ“‚ Repository Structure

```
â”œâ”€â”€ presentation/            # PDF and slide images
â”‚   â”œâ”€â”€ Transformer_Presentation.pdf
â”‚   â””â”€â”€ slide1.png
â”œâ”€â”€ notebooks/               # Jupyter Notebooks for each module
â”‚   â”œâ”€â”€ 01_positional_encoding.ipynb
â”‚   â”œâ”€â”€ 02_self_attention.ipynb
â”‚   â””â”€â”€ 03_transformer_full.ipynb
â”œâ”€â”€ src/                     # PyTorch implementations
â”‚   â”œâ”€â”€ positional_encoding.py
â”‚   â”œâ”€â”€ self_attention.py
â”‚   â””â”€â”€ transformer.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

### ğŸ“š Textbook Reference

* Prince, S. J. D. (2021). *Understanding Deep Learning*, Chapter 12: Transformers.

### ğŸ¥ Video References

* University of Waterloo Lecture on Transformers: [https://www.youtube.com/watch?v=OyFJWRnt\_AY](https://www.youtube.com/watch?v=OyFJWRnt_AY)
* 3Blue1Brown: "Transformers" [https://www.youtube.com/watch?v=wjZofJX0v4M&](https://www.youtube.com/watch?v=wjZofJX0v4M&)
* 3Blue1Brown: "Self-Attention Deep Dive" [https://www.youtube.com/watch?v=eMlx5fFNoYc&](https://www.youtube.com/watch?v=eMlx5fFNoYc&)
* Brief Overview: [https://www.youtube.com/watch?v=e9-0BxyKG10](https://www.youtube.com/watch?v=e9-0BxyKG10)
* Math of Self-Attention: [https://www.youtube.com/watch?v=UPtG\_38Oq8o&](https://www.youtube.com/watch?v=UPtG_38Oq8o&)
* Attention Mechanism Math: [https://www.youtube.com/watch?v=hsu\_5DyHj7I](https://www.youtube.com/watch?v=hsu_5DyHj7I)


---

## ğŸ“– VisÃ£o Geral (PortuguÃªs - Brasil)

Este repositÃ³rio oferece:

1. **ReproduÃ§Ã£o** da arquitetura Transformer do artigo seminal [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762) usando PyTorch.
2. **ExplicaÃ§Ãµes detalhadas** de cada componente (Multi-Head Self-Attention, CodificaÃ§Ã£o Posicional, subcamadas Feed-Forward, ConexÃµes de ResÃ­duo, NormalizaÃ§Ã£o de Camada).
3. **Notebooks Jupyter** com cÃ©lulas de cÃ³digo passo a passo, derivadas matemÃ¡ticas em LaTeX e visualizaÃ§Ãµes dos pesos de atenÃ§Ã£o.
4. **ApresentaÃ§Ã£o Complementar** em formato PDF; primeira lÃ¢mina mostrada acima.
5. **ReferÃªncias Adicionais**â€”aulas em vÃ­deo, explicaÃ§Ãµes populares e capÃ­tulo de livro para aprofundamento.

### ğŸ”— ApresentaÃ§Ã£o

* PDF: [Baixar apresentaÃ§Ã£o complementar](./presentation/Transformer_Presentation.pdf)

### ğŸš€ Como ComeÃ§ar

```bash
# Clonar repositÃ³rio
git clone https://github.com/B-Trindade/Attention-is-All-You-Need.git
cd Attention-is-All-You-Need

# Criar e ativar ambiente virtual 
python -m venv .venv
source .venv/bin/activate

# Instalar dependÃªncias
pip install -r requirements.txt
```

### ğŸ“‚ Estrutura do RepositÃ³rio

```
â”œâ”€â”€ presentation/            # PDF e imagens das lÃ¢minas
â”‚   â”œâ”€â”€ Transformer_Presentation.pdf
â”‚   â””â”€â”€ slide1.png
â”œâ”€â”€ notebooks/               # Notebooks Jupyter por mÃ³dulo
â”‚   â”œâ”€â”€ 01_positional_encoding.ipynb
â”‚   â”œâ”€â”€ 02_self_attention.ipynb
â”‚   â””â”€â”€ 03_transformer_full.ipynb
â”œâ”€â”€ src/                     # ImplementaÃ§Ãµes em PyTorch
â”‚   â”œâ”€â”€ positional_encoding.py
â”‚   â”œâ”€â”€ self_attention.py
â”‚   â””â”€â”€ transformer.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

### ğŸ“š Livro de ReferÃªncia

* Prince, S. J. D. (2021). *Understanding Deep Learning*, CapÃ­tulo 12: Transformers.

### ğŸ¥ VÃ­deos de ReferÃªncia

* Aula Universidade de Waterloo: [https://www.youtube.com/watch?v=OyFJWRnt\_AY](https://www.youtube.com/watch?v=OyFJWRnt_AY)
* 3Blue1Brown: "Transformers" [https://www.youtube.com/watch?v=wjZofJX0v4M&](https://www.youtube.com/watch?v=wjZofJX0v4M&)
* 3Blue1Brown: "AutoatenÃ§Ã£o em Detalhes" [https://www.youtube.com/watch?v=eMlx5fFNoYc&](https://www.youtube.com/watch?v=eMlx5fFNoYc&)
* VisÃ£o Breve: [https://www.youtube.com/watch?v=e9-0BxyKG10](https://www.youtube.com/watch?v=e9-0BxyKG10)
* MatemÃ¡tica da AutoatenÃ§Ã£o: [https://www.youtube.com/watch?v=UPtG\_38Oq8o&](https://www.youtube.com/watch?v=UPtG_38Oq8o&)
* MatemÃ¡tica de Mecanismos de AtenÃ§Ã£o: [https://www.youtube.com/watch?v=hsu\_5DyHj7I](https://www.youtube.com/watch?v=hsu_5DyHj7I)

### âœ¨ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor, abra *issues* ou *pull requests* para sugerir melhorias, correÃ§Ãµes de bugs ou novas funcionalidades.

### ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a **MIT License** â€” veja o arquivo [LICENSE](LICENSE) para mais detalhes.
