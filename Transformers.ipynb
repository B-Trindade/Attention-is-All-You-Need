{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97890786",
   "metadata": {},
   "source": [
    "# Attention is All You Need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebcef81",
   "metadata": {},
   "source": [
    "Trabalho complementar ao seminário apresentado no Laboratório de Engenharia de Computação Científica (LECC), pertencente ao Programa de Engenharia de Sistemas e Computação (PESC) - UFRJ. \n",
    "\n",
    "Aqui temos como objetivo destrinchar e entender por completo o modelo de Encoder-Decoder que introduziu a arquitetura de Transformers na literatura, originalmente proposto em:\n",
    "\n",
    "> [Attention is All You Need](https://arxiv.org/abs/1706.03762) - [Vaswani, A. et al. *Conference on Neural Information Processing Systems (NeurIPS)*, 2017]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717a919e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cb239d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b111fe56",
   "metadata": {},
   "source": [
    "Alguns comentários utilizam a extensão *Better Comments* (disponível no editor VS Code) para destacar informações, trazer questionamentos e expressar mais sem poluir o código. Seu uso é recomendado para melhor aproveitamento do material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a76658f",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f083c321",
   "metadata": {},
   "source": [
    "##### Input Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69297acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    TODO: Add docstring\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26e58c4",
   "metadata": {},
   "source": [
    "##### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad3072",
   "metadata": {},
   "source": [
    "Positional Encoding será uma função calculada **apenas uma vez** por posição. As senóides garantem que não ocorrerá repetição, uma vez que alterna a frequência do sinal conforme a feature!\n",
    "\n",
    "\\begin{equation} \\nonumber\n",
    "    \\textrm{PE}(\\textrm{pos}, 2i) = \\sin\\Big(\\frac{\\textrm{pos}}{10000^{ \\frac{2i}{d_{\\textrm{model}}} }}\\Big)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation} \\nonumber\n",
    "    \\textrm{PE}(\\textrm{pos}, 2i+1) = \\cos\\Big(\\frac{\\textrm{pos}}{10000^{ \\frac{2i}{d_{\\textrm{model}}} }}\\Big)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b5441",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    TODO: Verificar docstring\n",
    "    Implementa a codificacao posicional do Transformer\n",
    "    A codificacao posicional e uma matriz de dimensao (seq_len x d_model)\n",
    "    onde cada linha representa a codificacao de uma palavra na sequencia\n",
    "    A codificacao e feita utilizando funcoes trigonométricas, onde a posicao\n",
    "    da palavra e representada por um vetor de dimensao (1 x d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #* Matriz de dimensão (seq_len x d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "\n",
    "        # Vetor de posição da palavra (seq_len x 1)\n",
    "        pos = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) #? torch.float16 -> mais eficiente?\n",
    "        # Para maior estabilidade numerica utiliza-se o log \n",
    "        denominator = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Aplica trigonometricas\n",
    "        pe[:, 0::2] = torch.sin(pos * denominator)\n",
    "        pe[:, 1::2] = torch.cos(pos * denominator)\n",
    "\n",
    "        #* Transforma PE em um tensor de dim (batch_size x seq_len x d_model)\n",
    "        pe = pe.unsqueeze(0) # (1 x seq_len x d_model)\n",
    "        # Garante que o tensor seja salvo junto com o estado do modelo\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # PE nao e aprendido, nao requer gradiente\n",
    "        x += self.pe[:, :x.shape[1], :].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383856e3",
   "metadata": {},
   "source": [
    "##### Normalização"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310eab94",
   "metadata": {},
   "source": [
    "\n",
    "Como a distribuição de inputs nas camadas mais profundas está sujeita a mudanças bruscas devido a aprendizados em camadas mais razas, a Normalização é necessária para garantir que, apesar de ainda ocorrer mudanças na distribuição pelos mesmo motivos, a média ($\\mu$) e variância ($\\sigma$) permenecerão as mesmas!\n",
    "\n",
    "\\begin{equation} \\nonumber\n",
    "    \\mu = \\frac{1}{|H|}\\sum_{i \\in H}{h_i} \\quad , \n",
    "    \\quad \\sigma = \\sqrt{\\frac{1}{|H|} \\sum_{i \\in H}{(h_i - \\mu)^2} }\n",
    "\\end{equation}\n",
    "\n",
    "Normalizando:\n",
    "\n",
    "\\begin{equation} \\nonumber\n",
    "    \\hat{h}_j = \\frac{h_j - \\mu}{\\sigma_j + \\epsilon} \\quad \\longrightarrow \\quad \\hat{h}_j =  \\gamma \\frac{h_j - \\mu}{\\sigma_j + \\epsilon} + \\delta\n",
    "\\end{equation}\n",
    "\n",
    "Isso aumenta o tamanho do modelo, uma vez que a normalização ocorre a cada ativação numa camada, contudo, os benefícios observados são:\n",
    "- **Estabilização da _Forward Propagation_**: se o offset \\delta é inicializado com 0s e \\gamma com 1s, então a variância irá aumentar de forma linear com as camadas. O aprendizado inicial será mais lento, porém a rede aprende \\gamma e pode regular seu caminho efetivo\n",
    "- **Taxa de aprendizado ($\\eta$) maior**: Com a superfície de perda mais suave (e consequentemente seu gradiente) o gradiente também a mais dificilmente quebrado, assim, como a superfície é mais \"previsível\", pode-se utilizar taxas de aprendizado maiores \n",
    "- **Regularização**: Indiretamente, como a normalização de *batches* depende de estatísticas da própria leva (*batch*), o efeito prático é de pequenas variações nas ativações a cada iteração de treino, similar à introdução de ruído."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d9cfca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    \"\"\"\n",
    "    Epsilon (eps): parametro que evita divisao por 0 e estabiliza a normalizacao\n",
    "    Gamma: fator multiplicativo  (aprendido)\n",
    "    Delta: fator aditivo / offset (aprendido)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eps: float = 10**-6):\n",
    "        super().__init__()\n",
    "        # Epsilon evita divisao por 0 e valores muito elevados na normalizacao\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(1))\n",
    "        self.delta = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        keep dimension: .mean() cancela a dimensao por padrao, keepdim evita isso\n",
    "        \"\"\"\n",
    "        mean = x.mean(dim = -1, keepdim=True)\n",
    "        std = x.std(dim = -1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.delta "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa4696",
   "metadata": {},
   "source": [
    "##### Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63889983",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed Forward Block:\n",
    "\n",
    "    d_model -> d_ff -> d_model\n",
    "    d_model: Dimensao de entrada e saida\n",
    "    d_ff: Dimensao intermediaria\n",
    "    dropout: taxa de dropout\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff) # W1 e B1 (bias=True por padrao)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model) # W2 e B2\n",
    "        self.layer_norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feed Forward\n",
    "        x = self.dropout(torch.relu(self.linear1(x))) #d_model -> d_ff\n",
    "        return self.linear2(x) #d_ff -> d_model       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c3a9a",
   "metadata": {},
   "source": [
    "##### Multi-head Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acd453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Multihead Attention Block:\n",
    "    d_model: Dimensao de entrada e saida\n",
    "    n_heads: numero de cabecas\n",
    "    dropout: taxa de dropout\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # d_model deve ser divisivel por n_heads\n",
    "        assert d_model % n_heads == 0, f'd_model ({d_model}) must be divisible by n_heads ({n_heads})'\n",
    "        self.head_dim = d_model // n_heads # d_k \n",
    "\n",
    "        # WQ, WK, WV -> (d_model x d_model)\n",
    "        self.w_q = nn.Linear(d_model, d_model) # WQ e BQ (bias=True por padrao)\n",
    "        self.w_k = nn.Linear(d_model, d_model) # WK e BK\n",
    "        self.w_v = nn.Linear(d_model, d_model) # WV e BV\n",
    "\n",
    "        # Linear de saida -> (d_model x d_model)\n",
    "        self.w_out = nn.Linear(d_model, d_model) # WO e BO\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(self, query, key, value, dropout: nn.Dropout, mask=None):\n",
    "        \"\"\"\n",
    "        TODO: adicionar docstring\n",
    "        Implementa a atencao do Transformer\n",
    "        A atencao e feita utilizando a funcao softmax, onde a atencao e dada por:\n",
    "        softmax(QK^T / sqrt(d_k))V\n",
    "\n",
    "        Static method para permitir a chamada da funcao de atencao\n",
    "        sem precisar instanciar a classe.\n",
    "        \"\"\"\n",
    "\n",
    "        head_dim = query.shape[-1] # d_k\n",
    "\n",
    "        # Multiplica Q e K transposto\n",
    "        # (batch_size x n_heads x seq_len x head_dim) x (batch_size x n_heads x head_dim x seq_len)\n",
    "        attention_score = torch.matmul(query, key.transpose(-2, -1)) # -> (batch_size x n_heads x seq_len x seq_len)\n",
    "        attention_score /= math.sqrt(head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_score = attention_score.masked_fill(mask == 0, -1e9)\n",
    "        attention_score = torch.softmax(attention_score, dim=-1) # softmax(QK^T / sqrt(d_k))\n",
    "\n",
    "        if dropout is not None:\n",
    "            attention_score = dropout(attention_score)\n",
    "\n",
    "        # Aplicando a atencao no valor\n",
    "        # (batch_size x n_heads x seq_len x seq_len) x (batch_size x n_heads x seq_len x head_dim)\n",
    "        x = torch.matmul(attention_score, value) # -> (batch_size x n_heads x seq_len x head_dim)\n",
    "\n",
    "        return x, attention_score\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\"\n",
    "        .contiguous() -> garante que o tensor esteja na memoria contigua, i.e., \n",
    "         habilita o uso de .view() e concatenacao direta das cabecas\n",
    "        .transpose() -> troca a ordem das dimensoes do tensor\n",
    "        .view() -> transforma o tensor em uma nova forma\n",
    "        \"\"\"\n",
    "        # q, k, v: (batch_size x seq_len x d_model)\n",
    "        batch_size = q.size\n",
    "\n",
    "        query = self.w_q(q) # (batch_size x seq_len x d_model)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "        \n",
    "        # TODO: verificar se asserts estao corretos\t(especialmente o de batch_size)\n",
    "        assert query.size() == key.size() == value.size(), f'query ({query.size()}) != key ({key.size()}) != value ({value.size()})'\n",
    "        assert query.size() == (batch_size, -1, self.d_model), f'query ({query.size()}) != (batch_size, -1, d_model)'\n",
    "\n",
    "        # Transforma (batch_size x seq_len x d_model) em (batch_size x n_heads x seq_len x head_dim):\n",
    "\n",
    "        # Transpose se faz para que a dimensao de cabeca fique na segunda posicao\n",
    "        # e a dimensao de sequencia na terceira, de forma que cada cabeca veja todas as palavras\n",
    "        # da sequencia\n",
    "        query = query.view(query.shape[0], query.shape[1], self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # query, key, value: (batch_size x n_heads x seq_len x head_dim)\n",
    "\n",
    "        x, attention_score = MultiheadAttentionBlock.attention(self, query, key, value, self.dropout, mask)\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model) # (batch_size x seq_len x d_model)\n",
    "        return self.w_out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827074d9",
   "metadata": {},
   "source": [
    "##### Residual Connection (Skip Connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99332b73",
   "metadata": {},
   "source": [
    "Responsável por gerenciar a passagem de outputs entre blocos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e12825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual Connection:\n",
    "    Implementa a conexao residual do Transformer\n",
    "    A conexao residual e uma soma entre a entrada e a saida do bloco\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.layer_norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1aaaef",
   "metadata": {},
   "source": [
    "### Encoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498e1a00",
   "metadata": {},
   "source": [
    "\\begin{equation} \\nonumber\n",
    "Encoder = N \\times \\Big(\\textrm{Input} \\longrightarrow \\textrm{Multi-head SA} \\rightarrow \\textrm{Layer Norm} \\longrightarrow \\textrm{Feed Forward} \\rightarrow \\textrm{Layer Norm} \\Big) \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58866954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Block:\n",
    "    Implementa o bloco de codificacao do Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, self_attention: MultiheadAttentionBlock, feed_forward: FeedForwardBlock, dropout: float):\n",
    "        super().__init__()\n",
    "        self.attention = self_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        self.residual_connection = nn.ModuleList([\n",
    "            ResidualConnection(dropout),\n",
    "            ResidualConnection(dropout)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, src_mask=None):\n",
    "        \"\"\"\n",
    "        TODO: completar docstring\n",
    "        x: tensor de entrada (batch_size x seq_len x d_model)\n",
    "        src_mask: mascara de entrada para evitar que \n",
    "        palavras de padding sejam consideradas na atencao\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.residual_connection[0](x, lambda x: self.attention(x, x, x, src_mask))\n",
    "        x = self.residual_connection[1](x, self.feed_forward)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe473f90",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0282d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder:\n",
    "    Implementa o codificador do Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x) # Normaliza a saida do Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6815ac3",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92164328",
   "metadata": {},
   "source": [
    "### Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ae5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementa o bloco de decoder\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            self_attention: MultiheadAttentionBlock, \n",
    "            cross_attention: MultiheadAttentionBlock, \n",
    "            feed_forward: FeedForwardBlock,\n",
    "            dropout: float\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        self.residual_connections = nn.ModuleList([\n",
    "            ResidualConnection(dropout),\n",
    "            ResidualConnection(dropout),\n",
    "            ResidualConnection(dropout)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, y, src_mask = None, trg_mask = None):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention(x,x,x, trg_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention(x, y, y, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5243b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ):\n",
    "        super().__init__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
